---
title: "Key, Assignment 4"
author: "B. Brown"
date: "3/26/2017"
output: html_document
---

# Grading scheme

This was a tough one for a number of people. So I graded this more leniently. If your code was reasonable and you put in a good-faith effort, you got full points on a question.

Each question was worth 20 points.

You lost 2 points for not submitting an html file.

# Example answer
Here are some good answers from classmates, although most of this code comes directly from the lectures.

```{r setup, include=FALSE}
library(tidyverse)
library(class) # KNN
library(e1071) # SVM
library(kernlab) # kernel SVM
library(caret) # tuning
library(stringr)

train <- read_csv('https://raw.githubusercontent.com/idc9/stor390/master/data/human_activity_train.csv')

test <- read_csv('https://raw.githubusercontent.com/idc9/stor390/master/data/human_activity_test.csv')

# only consider walking upstairs vs downstairs
train <- train %>% 
    filter(activity == 2 | activity == 3)

test <- test %>% 
    filter(activity == 2 | activity == 3)


```

```{r subsample, eval=F}
# subsample the data
set.seed(8599)
train <- train[sample(x=1:dim(train)[1], size=200), ]
```

## Q1: KNN Test Set Error

### 1a.

```{r test_error}
# use these k values for KNN
k_values <- seq(from=1, to= 41, by=2)

# number of k values to check
num_k <- length(k_values)

# break the train/test data into x matrix and y vectors
train_x <- train %>% select(-activity)
train_y <- train$activity

test_x <- test %>% select(-activity)
test_y <- test$activity

test_error <- lapply(k_values, function(k) mean(knn(train_x, test_x, train_y, k) != test_y)) %>%
  unlist()
```

### 1b.

```{r cv}
# number of cross-validation folds
M <- 10
n <- dim(train)[1]

# create data frame to store CV errors
cv_error_df <- matrix(0, nrow=num_k, ncol=M) %>% 
    as_tibble() %>% 
    add_column(k=k_values)
colnames(cv_error_df) <- str_replace(colnames(cv_error_df), 'V', 'fold')

# seed for CV subsampling
set.seed(345)

# for each of the M folds
for(m in 1:M){
    
    # number of points that go in the cv train set
    n_cv_tr <- floor(n * (M-1)/M)
    
    # randomly select n_tr numbers, without replacement, from 1...n
    cv_tr_indices <- sample(x=1:n, size=n_cv_tr, replace=FALSE)
    
    # break the data into a non-overlapping train and test set
    cv_tr_data <- train[cv_tr_indices, ]
    cv_tst_data <- train[-cv_tr_indices, ]
    
    
    # break the train/test data into x matrix and y vectors
    # this formatting is useful for the knn() functions
    cv_tr_x <- cv_tr_data %>% select(-activity)
    cv_tr_y <- cv_tr_data$activity
    
    cv_tst_x <- cv_tst_data %>% select(-activity)
    cv_tst_y <- cv_tst_data$activity # turn into a vector
    
    # for each value of k
    for(i in 1:num_k){
        
        # fix k for this loop iteration
        k <- k_values[i]
        
        # get predictions on cv test data data
        cv_tst_predictions <- knn(train=cv_tr_x, # training x
                                  test=cv_tst_x, # test x
                                  cl=cv_tr_y, # train y
                                  k=k) # set k
        
        # compute error rate on cv-test data
        cv_tst_err <- mean(cv_tst_y != cv_tst_predictions)
        
        # store values in the data frame
        cv_error_df[i, paste0('fold',m)] <- cv_tst_err
    }
}


# compute the mean cv error for each value of k
cv_mean_error <- cv_error_df %>% 
    select(-k) %>% 
    rowMeans()

error_df <- tibble(k = k_values, cv_error = cv_mean_error, test_error = test_error)

error_df %>% 
    gather(key='type', value='error', cv_error, test_error) %>% 
    ggplot() +
    geom_point(aes(x=k, y=error, color=type, shape=type)) +
    geom_line(aes(x=k, y=error, color=type, linetype=type))
```

### 1c.

```{r best_k}
min_error <- error_df$test_error[1]
min_k <- 1

for(i in 1:num_k) {
    if(error_df$test_error[i] < min_error) {
        min_error <- error_df$test_error[i]
        min_k <- error_df$k[i]
    }
}
min_k
```

### 1d.

The CV error does not approximate the true test set error well for these values of K. The CV error could be approximated by a monotonic increase. However the test error does not show a similarly increasing trend. 

Cross validation gives a low optimal K and test set error gives a high optimal K---in fact K doesn't change the test set error much.

The cross validation error is not great at estimating the test error. Although both error curves increase on the higher end of k values, the cross validation curve lacks the inverse “U” structure that we see in the test error. This might be due to the fact that because most of the training data remains the same in each run of cross validation, the behavior is not too far from what we would expect when running on the training set in total.

## Q2: What happens when we change the number of folds?

### 2a.

```{r train_error}
# create data frame to store train error
train_error_df <- tibble(k = k_values, train_error = rep(0, num_k))

# calculate train error
for(i in 1:num_k){
    
    # fix k for this loop iteration
    k <- k_values[i]
    
    # get predictions on train data 
    train_predictions <- knn(train = train_x, # training x
                            test = train_x, # test x
                            cl = train_y, # train y
                            k = k) # set k
    
    # compute error rate on train data
    train_error <- mean(train_y != train_predictions)
    
    # store values in the data frame
    train_error_df[i, "train_error"] <- train_error
}
```

### 2b.

```{r plot}
error_df <- error_df %>% 
    add_column(train_error = train_error_df$train_error)

error_df %>% 
    gather(key='type', value='error', cv_error, test_error, train_error) %>% 
    ggplot() +
    geom_point(aes(x=k, y=error, color=type, shape=type)) +
    geom_line(aes(x=k, y=error, color=type, linetype=type))
```

### 2c.

```{r plot_function}
knn_tuning_error_plot <- function(train, test, k_cv, k_values, cv_seed=NA){
    # Returns the tuning error plots for KNN with the three tuning error curves
    # train, CV, and test error
    # train and test: are the train and test data
    # both are a data frame with the same column names
    # one column in named y which is the class labels
    # k_cv: is the number of cross validation folds
    # k_values: is the sequence of K values try for KNN
    # cv_seed: is the seed for the cross validation folds
    # returns a ggplot object    
    
    # set seed if it is given
    if(!is.na(cv_seed)){
        set.seed(cv_seed)
    }
    
    # number of k values to check
    num_k <- length(k_values)
    
    # break the train/test data into x matrix and y vectors
    train_x <- train %>% select(-y)
    train_y <- train$y
    
    test_x <- test %>% select(-y)
    test_y <- test$y
    
    #create data frame to store errors
    error_df <- tibble(k = k_values, test_error = rep(0, num_k), cv_error = rep(0, num_k), train_error = rep(0, num_k))
    
    # calculate test/train error
    for(i in 1:num_k){
        
        # fix k for this loop iteration
        k <- k_values[i]
        
        # get predictions on test/train data 
        test_predictions <- knn(train = train_x, # training x
                                test = test_x, # test x
                                cl = train_y, # train y
                                k = k) # set k
        train_predictions <- knn(train = train_x, # training x
                                test = train_x, # train x
                                cl = train_y, # train y
                                k = k) # set k
        
        # compute error rate on test/train data
        test_error <- mean(test_y != test_predictions)
        train_error <- mean(train_y != train_predictions)
        
        # store values in the data frame
        error_df[i, "test_error"] <- test_error
        error_df[i, "train_error"] <- train_error
        
    }
    
    # useful quantity
    n <- dim(train)[1]
    
    # create data frame to store CV errors
    cv_error_df <- matrix(0, nrow=num_k, ncol=k_cv) %>% 
        as_tibble() %>% 
        add_column(k=k_values)
    colnames(cv_error_df) <- str_replace(colnames(cv_error_df), 'V', 'fold')

    
    # for each of the M folds
    for(m in 1:k_cv){
        
        # number of points that go in the cv train set
        n_cv_tr <- floor(n * (k_cv-1)/k_cv)
        
        # randomly select n_tr numbers, without replacement, from 1...n
        cv_tr_indices <- sample(x=1:n, size=n_cv_tr, replace=FALSE)
        
        # break the data into a non-overlapping train and test set
        cv_tr_data <- train[cv_tr_indices, ]
        cv_tst_data <- train[-cv_tr_indices, ]
        
        
        # break the train/test data into x matrix and y vectors
        # this formatting is useful for the knn() functions
        cv_tr_x <- cv_tr_data %>% select(-y)
        cv_tr_y <- cv_tr_data$y
        
        cv_tst_x <- cv_tst_data %>% select(-y)
        cv_tst_y <- cv_tst_data$y # turn into a vector
        
        # for each value of k
        for(i in 1:num_k){
            
            # fix k for this loop iteration
            k <- k_values[i]
            
            # get predictions on cv test data data
            cv_tst_predictions <- knn(train=cv_tr_x, # training x
                                      test=cv_tst_x, # test x
                                      cl=cv_tr_y, # train y
                                      k=k) # set k
            
            # compute error rate on cv-test data
            cv_tst_err <- mean(cv_tst_y != cv_tst_predictions)
            
            # store values in the data frame
            cv_error_df[i, paste0('fold',m)] <- cv_tst_err
        }
    }
    
    # compute the mean cv error for each value of k
    cv_mean_error <- cv_error_df %>% 
        select(-k) %>% 
        rowMeans()
    
    # combine cv error with test/train
    error_df$cv_error <- cv_mean_error
    
    
    p <- error_df %>% 
        gather(key='type', value='error', cv_error, test_error, train_error) %>% 
        ggplot() +
        geom_point(aes(x=k, y=error, color=type, shape=type)) +
        geom_line(aes(x=k, y=error, color=type, linetype=type))
    
    return(p)
}
```

### 2d.

```{r plots}
train <- mutate(train, y = activity) %>% select(-activity)
test <- mutate(test, y = activity) %>% select(-activity)

knn_tuning_error_plot(train, test, 5, k_values, 345)
knn_tuning_error_plot(train, test, 10, k_values, 345)
knn_tuning_error_plot(train, test, 20, k_values, 345)
knn_tuning_error_plot(train, test, 50, k_values, 345)
```

## Q3: Nearest Centroid

### 3a.

```{r nc_function}
nearest_centroid <- function(train_x, train_y, test_x){
    # returns the predictions for nearest centroid on a test set
    # train_x and test_x: are the train/test x data
    # assume these are both numerical matrices with the same number of columns
    # train_y: is a vector of class labels for the training data
    # return a vector of predicted class labels for the test data
    
    obs_means <- train_x %>% 
        mutate(y = train_y) %>%
        group_by(y) %>% 
        summarise_all(mean) %>%
        select(-y)

    # compute the distance from each test point to the two class means
    dist_up <- apply(test_x, 1, function(x) sqrt(sum((x - obs_means[1, ])^2)))
    dist_down <- apply(test_x, 1, function(x) sqrt(sum((x - obs_means[2, ])^2)))

    
    # decide which class mean each test point is closest to
    dist <- tibble(dist_up = dist_up, dist_down = dist_down)
    dist <- dist %>%
        mutate(y_pred = ifelse(dist_up < dist_down, 2, 3)) %>% 
        mutate(y_pred = factor(y_pred))
    
    dist$y_pred
}
```

### 3b.

```{r nc_error}
# train error
nearest_centroid_train_y <- nearest_centroid(train_x, train_y, train_x)

mean(train_y != nearest_centroid_train_y)

# test error
nearest_centroid_test_y <- nearest_centroid(train_x, train_y, test_x)

mean(test_y != nearest_centroid_test_y)
```

## Q4: Linear SVM

```{r linear_svm, warning=F}
lin_svm_cv <- function(train_x, train_y, num_folds, class_metric) {
  
  train_y <- factor(train_y)
  C_values <- 10^seq(from=-5, to=5, by=1)
  trControl <- trainControl(method = "cv", number = num_folds)
  tune_grid <- expand.grid(C = C_values)
  tuned_svm <- train(x=train_x, y=train_y, method = "svmLinear",
                   tuneGrid = tune_grid, trControl = trControl, metric = class_metric)
  return(tuned_svm)
  
}


metrics <- c("Accuracy", "Kappa")
folds <- c(5, 10)

lin_svm_errors <- tibble(folds=rep(0, 4), metric=rep(0, 4), best_C = rep(0, 4), test=rep(0, 4), train=rep(0, 4))
counter <- 1

for(i in 1:2) {
  for(j in 1:2) {
    lin_svm_errors[counter, "folds"] <- folds[i]
    lin_svm_errors[counter, "metric"] <- metrics[j]
    tuned_svm <- lin_svm_cv(train_x, factor(train_y), folds[i], metrics[j])
    lin_svm_errors[counter, "best_C"] <- tuned_svm$bestTune
    test_predictions <- predict(tuned_svm, newdata = test_x)
    train_predictions <- predict(tuned_svm, newdata = train_x)
    lin_svm_errors[counter, "test"] <- mean(test_predictions != test_y)
    lin_svm_errors[counter, "train"] <- mean(train_predictions != train_y)
    counter <- counter + 1
  }
}

lin_svm_errors
```

## Q5: Radial Kernel SVM

```{r radial_svm, warning=F}
sigma_values <- 10^seq(from=-5, to=5, by=1)
C_values <- 10^seq(from=-5, to=5, by=1)

# specify tuning procedure
trControl <- trainControl(method = "cv", 
                          number = 5) 
tune_grid <- expand.grid(C = C_values, sigma = sigma_values)

tuned_svm_radial <- train(x=train_x,
                          y=train_y,
                          method = "svmRadial", 
                          tuneGrid = tune_grid, 
                          trControl = trControl, 
                          metric="Accuracy")

# optimal C = 0.001, sigma = 1

ksvmfit <- ksvm(y ~ ., 
              data=train, 
              C=0.001, 
              type='C-svc',
              kernel='rbfdot', 
              kpar=list(sigma=1), 
              scaled=FALSE,
              shrinking=FALSE)

# train error

train %>% 
    mutate(y_pred = predict(ksvmfit, newdata = train)) %>% 
    summarise(error = mean(y != y_pred))

# test error

test %>% 
    mutate(y_pred = predict(ksvmfit, newdata = test)) %>% 
    summarise(error = mean(y != y_pred))
```





