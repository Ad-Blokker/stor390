---
title: "Variable selection for linear models"
output: html_document
---

**Warning** these notes are note yet complete.

These notes are about variable selection for linear models focusing on prediction tasks. The reference for these notes is [**ISLR chapter 6**](http://www-bcf.usc.edu/~gareth/ISL/). The goal of these notes is to understand the intuition behind various variable selection techniques and be able to implement them in R.

These notes do not cover many of the statistical properties or all of the math underlying these algorithms. ISLR and other standard textbooks cover these techniques more rigorously. Understanding these techniques rigorously is important for doing better statistics and learning about more advanced material. Furthermore, if your primary purpose is inference (as opposed to prediction) you really should understand the statistical properties of these procedures.

```{r}
# you may need to install the leaps library
library(leaps)
```


# **Variable selection**

[Variable selection](https://en.wikipedia.org/wiki/Feature_selection) is the process of deciding which variables go into your model. It is also called *model selection* or *feature selection*. 

If your goal is prediction then you are likely doing variable selection to reduce

- overfitting (therefore get better predictive accuracy)
- computation (your algorithm might be really slow if you give it a lot of variables)
- dimension (high-dimensional data present all sorts of peculiarities)

If your goal is inference you are likely doing variable selection to understand which X variables are truly associated with your response Y. Recall the mantra: "correlation does not imply causation." Variable selection gets at something even more basic: is that correlation truly there.

For inference the math gets harder and more important. For prediction life is a little easier because [TODO]

Variable selection is about picking which subset of variables go into the model (i.e. variables 1, 3, 5, not 2, 4). There are two philosophical perspectives you might take: selecting a subset of variables or tuning a parameter. Both perspectives can be helpful in different cases.

Variable selection is hard because in theory you want to consider *every* subset of variables under consideration. If you have d variables then you have to consider $2^d$ subsets. This quickly turns into an end of the universe type calculation. Many of the variable selection procedures are approximations (hacks) to avoid an exponential calculation.

There are usually two ingredients to a variable selection procedure

- a metric (e.g. p-value, sum of squared residuals, test error)

- a way method to compare subsets of variables (e.g. backwards, forwards, stage-wise)


# **The data**

We will use the Capital Bike sharing data from 2011 (used in the previous lecture).

```{r, message=F, warning=F}
library(tidyverse)

# data from 2011
data <- read_csv('https://raw.githubusercontent.com/idc9/stor390/master/data/bikes_2011.csv')

# set variables to factors
data <- data %>% 
        mutate(season=factor(season),
               holiday=factor(holiday),
               workingday=factor(workingday),
               weathersit=factor(weathersit))


hour <- data %>% 
        select(cnt, hr) %>% 
        rename(hr_1 = hr)

d_max <- 10

for(d in 2:d_max){
    
    # name for new column to dadd
    var_name <- paste0('hr_', d)
    
    # add polynomial terms of hr to x data matrix
    hour[var_name] <- hour$hr_1^d
}


# hour <- hour %>% 
#    select(season, holiday, workingday, hr, holiday, weathersit, temp, atemp, hum, windspeed)

```


We are interested in predicting the number of bike counts based on a number of covariates (time of day, weather, etc). We could start with all the variables currently in the raw data set and do variable selection procedures. Additionally, we could **add transformed features** such as polynomial or interaction terms (see [previous lecture](https://idc9.github.io/stor390/notes/predictive_modeling/predictive_modeling.html)).



# Remove insignificant variables using p-values

Very simple one step procedure that makes use of statistical assumptions

1. Fit the linear regression with all variables.
2. Remove variables whose p-values are above 0.05. (Q: what is the hypothesis test?)
3. Refit the linear model with remaining variables

```{r}

# fit linear model with all coefficients
lin_reg <- lm(cnt ~., hour)
summary(lin_reg)
```

Looking at the print out we see only 5 of the hour polynomial terms are significant at alpha = 0.05. For example, the raw hour (`hr_1`) is not statistically significant. Note that this result is **not** saying the raw hour has no relationship with counts. It's saying something more like: in the presence of the other variables `hr_1` does not appear to [TODO]


Now we want to programmatically extract with variables are statistically significant.

```{r}
# you can find the p-values for each variable in the coefficients field of summary
p_vals <- summary(lin_reg)$coefficients[,4]

# decide which variables to keep
variables_to_keep <- names(p_vals)[p_vals < 0.05]

# notice the first term variables_to_keep is Intercept -- et's ignore this. 
variables_to_keep <- variables_to_keep[2:length(variables_to_keep)]

# get rid of the rest of the variables
hour_reduced <- hour[, c('cnt', variables_to_keep)]
```

Now that we have eliminated a bunch of variables we re-fit the linear model with the remaining variables

```{r}
lin_reg <- lm(cnt ~. , data=hour_reduced)
summary(lin_reg)
```

# Iteratively remove insignificant variables using p-values

When we refit the linear model above after removing a bunch of variables it's possible that some of the remaining variables are not statistically significant in the new fit [TODO: is this actually possible??] We can therefore adapt the above procedure by iterating until every variable left in the model is statistically significant.


1. Fit the linear regression with all variables.
2. Remove variables whose p-values are above 0.05.
3. Refit the linear model with remaining variables.
4. Repeat until all variables in remaining model are significant.

# Bias-variance tradeoff

The more variables you put into the model the better the training error will be. However, the model becomes more complex in some sense. Many variable selection procedures attempt to balance this trade-off between training accuracy and model complexity in some way. This is the so called [bias-variance tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html).


# Subset selection algorithms

For more details on these algorithms read [ISLR chapter 6.1](http://www-bcf.usc.edu/~gareth/ISL/). These notes are meant to be a high level overview of these algorithms.

The backwards, forwards and step-wise selection are all hierarchical procedures. They use one criterion to compare models that have the same number of variables (i.e. all models with 4 of the 12 possible variables). They then use another criterion to compare models with different number of variables.

The Residual Sum or Squares (RSS) or $R^2$ statistics are both measures of how well a model fits the data. Note that RSS is just the least squares training error. These metrics make sense to compare models with the same number of variables. However, they are not good to compare models with different numbers of variables (i.e. a model with 4 variables vs. a model with 6 variables). The model with more variables essentially has an unfair advantage. 

Suppose we have $d$ variables under consideration. These subset selection procedures
first narrow down all $2^d$ possible models to $d$ possible models by using $R^2$ to compare models the same number of variables. Then we pick one of these $d$ models using one of the following statistics

- cross-validation predictive performance
- Mallow's $C_p$ statistics
- Akaike information criterion (AIC)
- Bayesian information criterion (BIC)
- adjusted $R^2$

The latter 4 of these statistics are all ways of balancing the number of variables in the model with the training error. For example if we have $n$ observations, $d$ variables then

$$ \text{AIC} = n \log\left( \frac{RSS}{n} \right) + 2d$$
I.e. AIC is small when the RSS is small and when $d$ is small. We can use AIC to compare our $d$ "best" models for each given number of variables. 

All of these statistics are built on certain statistical assumptions about the data. It's worth understanding these quantities and their properties in more depth (if you do a PhD in statistics you may want to tattoo these formulas on your arm before your first year exams.)




## Best subset

Best subset selection does exactly what was descried above. It looks at all $2^d$ possible models. It first narrows the $2^d$ models down to $d$ models by comparing models with he same number of variables. It then uses something like AIC to compare the $d$ remaining models to select the one best one.


Let's run best subset selection with adjusted $R^2$ to select the best model. Using the `leaps` library,
```{r}
# need to pass in y vector and results matrix separatly to leaps package
y <- hour$cnt
X <- select(hour, -cnt)

# run best subset selection
# The documentation for leaps is not great
best_subset <- leaps(x=X, y=y, method='adjr2', nbest=1)
best_subset
```

Now that we have run best subset selection let's pick the best model

```{r}
# using adjusted R^2 pick the size of the best model
d_best <- which.min(best_subset$adjr2)

# which variables should be included in the best model
variables_to_keeep <- best_subset$which[d_best, ]

# subset out the hours matrix
# the first T below is for the cnt varible
hour_reduced <- hour[, c(T, variables_to_keeep)]

hour_reduced
```

**Warning**: best subset selection is rarely used in practice because it requires an exponential computation!


## Backwards selection

Backwards selection is a [greedy algorithm](https://en.wikipedia.org/wiki/Greedy_algorithm) that is meant to approximate best subset selection. Backwards selection is actually used in practice.

Start with the "full model" (i.e. the model with all $d$ variables). Consider all $d$ models with one variable removed and select the best one of these using RSS (i.e. remove one variable). Of the remaining variables, consider all further restricted models with one more variable removed and pick the best one again using RSS. Repeat this process until you have a sequence of $d$ models each with a distinct number of variables. 

Now that we have $d$ models, use something like AIC to select the best one model.

Note that this process requires checking out $k$ models for $k = 1, \dots, d$. This turns into a quadratic (i.e. $d^2$ calculation) which is much better than an exponential $2^d$ calculation. 

```{r}
reg <- regsubsets(cnt ~., data=hour, nbest=1, method='backward')

reg$which
```
[TODO: finish code]



# Regularization and the Lasso

[Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)) and particularly L1 regularization (also known as [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics))) are some of the most popular ideas in statistics/machine learning. Most statistical models are phrased as optimization problems: let's find the model that minimizes this thing we don't like (i.e. a [loss function](https://en.wikipedia.org/wiki/Loss_function)). 



## L2 regularization
Recall from the previous lecture that linear regression minimizes the sum of squared residuals. Suppose we have $n$ observations in $d$ dimensions. Let $x_{ik}$ be the k$th$ variable of the i$th$ observation. Let $y_i$ be the i$th$ response. Let $\beta_0$ be the linear regression constant term and $\beta_1, \dots, \beta_d$ be the linear regression coefficients. Then liner regression solves


$$ \min_{\mathbf{\beta}} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_{i1} - \dots -  \beta x_{id})^2$$
The quantity $y_i - \beta_0 - \beta_1 x_{i1} - \dots -  \beta x_{id}$ is small if $ \beta_0 + \beta_1 x_{i1} + \dots + \beta x_{id}$ is close to $y_i$. The intuition behind minimizing the residuals should be fairly straight forward.

Now let's add another term to the above optimization function to penalize large values of $\beta_j$. We now solve the following optimization problem (we fix $\lambda >0$ ahead of time, more on how to select $\labmda$ later)

$$ \min_{\mathbf{\beta}} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_{i1} - \dots -  \beta x_{id})^2 + \lambda \sum_{j=1^d} \beta_j^2$$

**The intuition** is that we suspect large values of the $\beta_j$ coefficients is bad. 




