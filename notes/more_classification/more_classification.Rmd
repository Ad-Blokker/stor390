---
title: "**More classification**"
subtitle: "SVM, kernels, classification metrics"
author: "[STOR 390](https://idc9.github.io/stor390/)"
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
---


The [support vector machine](https://en.wikipedia.org/wiki/Support_vector_machine) (SVM) is one of the most popular and effective classification algorithms. Like [nearest centroid](https://idc9.github.io/stor390/notes/classification/classification.html), SVM is a linear classifier. Recall we were able to [turn linear regression into non-linear regression](https://idc9.github.io/stor390/notes/predictive_modeling/predictive_modeling.html) by explicitly adding more variables using non-linear variable transformations (e.g. polynomial terms). Similarly, we can turn a linear classifier into a non-linear classifier by adding non-linear terms.

There is a way of automatically adding non-linear variables into a linear classifier by doing something called the [kernel trick](https://en.wikipedia.org/wiki/Kernel_method).


This lecture will cover

- maximal margin classifier (aka hard margin SVM)
- soft margin SVM
- Kernel SVM
- other classification metrics than just misclassification error

Hard margin SVM is useful to help understand soft margin SVM, but is not commonly used (although sometimes hard margin SVM works well for very high-dimensional data). These notes present the intuition behind soft margin SVM and point to reading for the mathematical details. Classification accuracy is an important metric, but there are other classification metrics you should be aware of.

```{r, message=F, warning=F}
# this library implements SVM
library(e1071)

library(mvtnorm)
library(tidyverse)

# some helper functions I wrote
source('synthetic_distributions.R')
source('svm_fun.R')
```

# **Takeaway and Resources**

The point of this lecture is to introduce the intuitive ideas behind SVM and kernels and show you how to use them in R. The math underlying these algorithms is explained well in the resources listed below. 

For more details see 

- [Andrew Ng's notes on SVM](http://cs229.stanford.edu/notes/cs229-notes3.pdf)
- [ISLR chapter 9](http://www-bcf.usc.edu/~gareth/ISL/) also [Elements of Statistical Learning chapter 12](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf)
- [Foundations of Machine Learning chapter 4](http://www.cs.nyu.edu/~mohri/mlbook/) provides a clear explanation of the optimization problem and a theoretical perspective favored in the CS community (generalization bounds).

# **Maximal Margin**

The maximal margin (MM) classifier (also known as *hard margin support vector machine* assumes the two classes of data are [linearly separable](https://en.wikipedia.org/wiki/Linear_separability) i.e. we can draw a line where every data point in the first class is on one side and every point in the other class is on the other side of the line.


```{r, echo=F}
data <- two_class_guasssian_meatballs(n_pos=200, n_neg=200,
                                      mu_pos=c(4,0), mu_neg=c(-4,0),
                                      sigma_pos=diag(2), sigma_neg=diag(2),
                                      seed=103)

ggplot(data=data) +
    geom_point(aes(x=x1, y=x2, color=y, shape=y)) +
    theme(panel.background = element_blank()) +
    lims(x=c(-8, 8), y=c(-8, 8))
```


[Recall](https://idc9.github.io/stor390/notes/classification/classification.html#nc_is_a_linear_classifier) a linear classifier makes predictions by separating the data space into two regions by a hyperplane. When the data are two dimensional, as above, this means a linear classifier draws a line in the plane. 

In the case the data are separable a reasonable aspiration is to have the separating hyperplane lie between the two classes i.e. one class is totally on one side of the hyperplane and the other class is totally on the other side. Suppose we call the distance between the hyperplane and the nearest data point the *margin*. 

The MM hyperplane is allergic to data points; MM seeks to find the hyperplane that is furthest away from the nearest data point. In other words MM finds the hyperplane that maximizes the margin.


```{r, echo=F, warning=F}
svmfit <- svm(y ~.,
              data=data,
              scale=FALSE,
              type='C-classification',
              shrinking=FALSE,
              kernel='linear',
              cost=1e4)


# get svm direction
svm_params <- get_svm_parmas(svmfit)
w_svm <- svm_params['w'][[1]]
b_svm <- svm_params['b'][[1]]

# add in support vector indicator column
data_svm <- data %>% 
    mutate(support_vector = 1)
data_svm[svmfit$index, 'support_vector'] <- 2


# fudge factor to make the plots work -- not sure why I need this
w_svm <- 1.05 * w_svm

# support vectors and corresponding points on the separating hyperplane
sv1 <- svmfit$SV[1, ]
sv_line1 <- sv1 -  w_svm / norm(matrix(w_svm)) ^ 3

sv2 <- svmfit$SV[2, ]
sv_line2 <- sv2 -  w_svm / norm(matrix(w_svm)) ^ 3

sv3 <- svmfit$SV[3, ]
sv_line3 <- sv3 +  w_svm / norm(matrix(w_svm)) ^ 3


# plot svm 
ggplot(data=data_svm) +
    geom_point(aes(x=x1, y=x2, color=y, shape=y, size=support_vector)) +
    geom_abline(slope=-w_svm[1]/w_svm[2], intercept = b_svm/w_svm[2]) +
    geom_abline(slope=-w_svm[1]/w_svm[2], intercept = (b_svm + 1)/w_svm[2], linetype = 2) +
    geom_abline(slope=-w_svm[1]/w_svm[2], intercept = (b_svm - 1)/w_svm[2], linetype = 2) +
    geom_segment(aes(x=sv1[1], y=sv1[2], xend=sv_line1[1], yend = sv_line1[2]), arrow=arrow(ends="both", length=unit(.2,"cm")), color='red') +
    geom_segment(aes(x=sv2[1], y=sv2[2], xend=sv_line2[1], yend = sv_line2[2]), arrow=arrow(ends="both", length=unit(.2,"cm")), color='red') +
    geom_segment(aes(x=sv3[1], y=sv3[2], xend=sv_line3[1], yend = sv_line3[2]), arrow=arrow(ends="both", length=unit(.2,"cm")), color='red') +
    theme(panel.background = element_blank()) +
    guides(size=FALSE) +
    lims(x=c(-8, 8), y=c(-8, 8))
```

In the plot above the MM separating hyperplane is shown as the solid line. The red arrows show the margin width -- the distance between the separating hyperplane and the nearest point. The dashed lines are the so called *marginal hyperplanes*. These marginal hyperplanes are parallel to the separating hyperplane. The highlighted points are called *support vectors* and are the points that are closest to the separating hyperplane (all three are equidistant to the separating hyperplane and lie on the marginal hyperplanes). 

The support vectors play an important role in understanding MM and SVM in general. The MM normal vector and intercept depend only on the support vectors -- none of the other data points. In contrast, nearest centroid depends on all the data points.

We can write the maximal margin classifier as an optimization problem: of all hyperplanes that separate the two classes, find the one that maximizes the margin. All of the aforementioned resources discuss how to write this as a math problem then solve the math problem.


# **Soft Margin Support Vector Machine**


Linear separability is a strong assumption that is typically not true for data sets; often the two classes cannot be separating by a hyperplane.

```{r, echo=F}
data <- two_class_guasssian_meatballs(n_pos=200, n_neg=200,
                                      mu_pos=c(1,0), mu_neg=c(-1,0),
                                      sigma_pos=diag(2), sigma_neg=diag(2),
                                      seed=103)

ggplot(data=data) +
    geom_point(aes(x=x1, y=x2, color=y, shape=y)) +
    theme(panel.background = element_blank())
```

Soft margin [support vector machine](https://en.wikipedia.org/wiki/Support_vector_machine) (which we will call just SVM) is an adaptation of the maximal margin classifier that is cool with data points lying on the wrong side of the separating hyperplane.

Keep the following intuition in mind for this section. A good linear classifier should aim to put points

- on the correct side of the separating hyperplane far away from the separating hyperplane
- on the wrong side of the separating hyperplane close to the separating hyperplane



## Soft margin SVM (words)

See [ISLR](http://www-bcf.usc.edu/~gareth/ISL/) chapter 9 for details about SVM.

Maximizing the margin is about putting points as far on the correct side of the separating hyperplane as possible. Soft margin SVM allows some "bad" points to be on the wrong side of the separating hyperplane, but penalizes these bad points. SVM then wants to put the remaining "good" points as far on the correct side of the margin as possible.

SVM thus has two competing objects: maximize the margin, but penalize disobedient points. SVM has a devil one on shoulder and angle on the other shoulder. Like many machine learning algorithms, SVM tries to strike a balance between two competing objectives. 

In order to strike this balance between the two competing objectives, SVM comes with a tuning parameter, $C >0$. This tuning parameter, $C$, controls how much SVM cares about the "bad" points. When $C$ is large SVM tries very hard to put put every training point on the correct side of the separating hyperplane. When $C$ is small SVM has more chill about points being on the wrong side of the separating hyperplane. 

All of the above mentioned resources discuss how to write this as a math problem then solve the math problem. 

## Tuning SVM

The plots below show the SVM fit for three different values of C (C=100, .01, .0001). The solid line shows the SVM separating hyperplane. The dashed lines show the SVM marginal hyperplanes which play an important role in the inner workings of SVM (these do not have the same interpretation as the marginal hyperplanes for the MM classifier). 

```{r, echo=F, warning=F}
make_svm_plot_2d(data, C=1e3)
make_svm_plot_2d(data, C=1e-2)
make_svm_plot_2d(data, C=1e-3)
```

Notice that as $C$ decreases more points are misclassified. This is another example of the bias-variance tradeoff (recall chapter 2 from ISLR). Large values of $C$ make the classifier try really hard to not misclassify anyone -- the so called low bias, high variance regime (leads to overfitting). Small values of $C$ are more ok with misclassified points -- high bias, low variance. As with many things in life the key is to find the right balance. 

The best value of $C$ is typically selected using [cross-validation](https://idc9.github.io/stor390/notes/cross_validation/cross_validation.html).

# **SVM code**

Unlike linear regression, or nearest centroid, SVM cannot be solved in closed form (i.e. there is no simple formula using the data to get the normal vector and intercept). Fitting SVM requires solving a [quadratic program](https://en.wikipedia.org/wiki/Quadratic_programming) which you could probably do after taking one course in optimization. Luckily for us, people have already coded up SVM.

We will use the [e1071](https://cran.r-project.org/web/packages/e1071/e1071.pdf) package to implement SVM. The [kernlab](https://cran.r-project.org/web/packages/kernlab/kernlab.pdf) package also implements SVM in R, but for some reason I prefer `e1071`...


First let's sample some training data and some test data (same as the data shown above).

```{r}
# this function comes from the synthetic_distributions.R package
train <- two_class_guasssian_meatballs(n_pos=200, n_neg=200,
                                       mu_pos=c(1,0), mu_neg=c(-1,0),
                                       sigma_pos=diag(2), sigma_neg=diag(2),
                                       seed=103)

train

test <- two_class_guasssian_meatballs(n_pos=1000, n_neg=1000,
                                       mu_pos=c(1,0), mu_neg=c(-1,0),
                                       sigma_pos=diag(2), sigma_neg=diag(2),
                                       seed=634)

```


Before fitting SVM we have to decide on the value of the tuning parameter to use -- let's stick with $C = 10$ for now. The `svm()` function is from the `e1071` package. The function might look intimidating -- there are an annoying number of arguments being set discussed below. Pay attention to the three lines with comments next to them -- these are the important lines.
```{r}
# fit SVM
svmfit <- svm(y ~ ., # R's formula notation
              data=train, # data frame to use
              cost=10, # set the tuning paramter
              scale=FALSE,
              type='C-classification',
              shrinking=FALSE,
              kernel='linear') 
```

**main arguments**
- `data=train` says fit SVM using the data stored in the `train` data frame. 

- The `svm()` function uses R's formula notation. Recall from linear regression `y ~ .` means fit `y` on all the rest of the columns of data. We could have equivalently used `y ~ x1 + x2`. 

- `cost = 10` fixes the tuning parameter $C$ to 10. The tuning parameter $C$ is also sometimes called a *cost* parameter.

- `shrinking=FALSE` I'm not sure what this does, but I don't want anything extra to happen so I told it to stop.

`

**Other arguments**
Check out the documentation to read more about the arguments (i.e. run `?svm`). The `svm()` function can do a lot of stuff which is why it has so many arguments. 

- `scale = FALSE` says **please do not center and scale** our data. `svm()` applies some [preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html) to the data by default. While preprocessing (e.g. center and scale) is often a good thing to do, I strongly disagree with making this the default behavior.

- `type='C-classification'` tells `svm()` to do classification. It turns out SVM can be used to do other things than classification](http://kernelsvm.tripod.com/). 

- `kernel='linear'` says do linear SVM. The `svm()` function can do kernel SVM (discussed below). 

Now that we have fit SVM let's see what we have. Use the `names()` function to see what the `svmfit` object has stored.

```{r}
names(svmfit)
```
You can read about these in the `?svm()` documentation. One value that might be of interest is the predictions for the training points i.e. (also called fitted values)
```{r}
svmfit$fitted[1:5]
```
To use SVM to make prediction on new data we can use the `predict` function i.e.

```{r}
# this is equivalent to svmfit$fitted
train_predictions <- predict(svmfit, newdata = train)
train_predictions[1:5] 
```

Ok let's see how SVM did on the training data

```{r}
train %>% 
    mutate(y_pred= train_predictions) %>% 
    summarise(error = mean(y != y_pred))
```
And how about the test set?
```{r}
test %>% 
    mutate(y_pred = predict(svmfit, newdata = test)) %>% 
    summarise(error = mean(y != y_pred))
```

In reality we would have first done cross-validation to select $C$. 

## software tangent

The fact that there are good, open source implementations of SVM is something you should take a minute to appreciate. The `e1071` package is written in R and does not do any heavy lifting; it calls the [LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) package which is written in C. C has some benefits over R -- it is typically faster and allows for better memory handling. C, however, is harder to write code it. 

Many popular machine learning algorithms are coded in a lower level language like C. They then wrapped in a higher level language like R or Python so they can be used with minimal headache.

This means some kind soul (more likely wretched grad student) took the time to code up

a) a good implementation of SVM in C and then release it to the public
b) a package in R (and Python and many other languages) so that us data scientists don't have to learn C to aforementioned C implementation of SVM

This saves you, the user, a lot of time and money. A machine learning PhD student could probably do all of this themselves, but it would likely take them a couple weeks to get the code working correctly and quickly. You could also hire someone to do this, but it would be pricey. 

These open source software implementations mean that instead of spending weeks and/or lots of money, you have access to a quality implementation of SVM in a matter of seconds for free. I would guess open source software is a big, unsung driver behind the explosion of big data.

Like all things in life, there are tradeoffs to using open source software.

- There can be bugs in open source software -- no one has a financial incentive to thoroughly test the code. 
    - Of course there can be bugs in professional software. Also the more people use a piece of software, the more likely a bug is to be caught.
    
- The documentation for open source software can be poor (again no financial incentive to make it clear).

- You don't have control over design choices.
    - Your favorite SVM package has bells and whistles 1 - 5, but you want bell and whistle number 6? You're [SOL](http://www.urbandictionary.com/define.php?term=SOL) since you didn't write the source code.



# **Non-linear classifiers**

Approximating complex patterns with simple patterns is a very powerful idea in mathematics. Often this comes down to approximating a non-linear thing with a linear thing (curve are hard, lines are easy!) Linear regression is a very effective regression algorithm even though many relationships are not in fact linear. Similarly, linear classifiers (nearest centroid, LDA, SVM, etc) are very effective classifiers even though many classification problems are not linear. Sometimes, however, the linear approximation is not good enough and we need a more sophisticated pattern. 




## Explicit variable transformation

[Recall](https://idc9.github.io/stor390/notes/predictive_modeling/predictive_modeling.html) we were able to turn linear regression into non-linear regression by adding transformations of the original variables. For example, instead of linear regression of $y$ on $x$ (`y ~ x`) we added polynomial $x$ terms (e.g. `y ~ x + x^2 + x^3`). The resulting model is linear in the added variables, but non-linear in the original variables. We could have added any function of $x$ we wanted e.g. $e^{4.3 x}, \sin(8.3 x), \max(x, 1),$ etc. This idea also works for classification.

We can use non-linear variable transformations to turn a linear classifier into a non-linear classifier. 



```{r}
# some training data
train <- gmm_distribution2d(n_neg=200, n_pos=201, mean_seed=238, data_seed=1232)

# test grid
test_grid <- expand.grid(x1 = seq(-5, 5, length = 100),
                         x2 = seq(-5, 5, length = 100)) %>% 
            as_tibble()
```


```{r, echo=F}
ggplot(data=train)+
    geom_point(data=train, aes(x=x1, y=x2, color=y, shape=y)) +
    theme(panel.background = element_blank())

```

```{r}
# fit SVM
svm_linear <- svm(y ~ ., 
                  data=train,
                  scale=FALSE,
                  type='C-classification',
                  shrinking=FALSE,
                  kernel='linear', 
                  cost=10)

grid_predictions <- predict(svm_linear, newdata = test_grid)
```


```{r, echo=F}
# plot predictions for linear svm
test_grid %>% 
    mutate(y_pred = grid_predictions) %>% 
    ggplot() +
    geom_point(aes(x=x1, y=x2, color=y_pred), alpha=.3) +
    geom_point(data=train, aes(x=x1, y=x2, color=y, shape=y)) +
    theme(panel.background = element_blank(),
          legend.position="none") +
    ggtitle('linear SVM fit')
```

Now let's manually add some polynomial terms.
```{r}
# add polynomial terms to 
train_poly <- train %>% 
                mutate(x1_sq = x1^2, x1x2 = x1*x2, x2_sq = x2^2)


test_grid_poly <- test_grid %>% 
                    mutate(x1_sq = x1^2, x1x2 = x1*x2, x2_sq = x2^2)


# fit SVM
svm_poly <- svm(y ~ ., 
                  data=train_poly,
                  scale=FALSE,
                  type='C-classification',
                  shrinking=FALSE,
                  kernel='linear', 
                  cost=10)

grid_poly_predictions <- predict(svm_poly, newdata = test_grid_poly)


# The above is equivalent to using R's formula notation
# svm_poly <- svm(y ~ x1 + x2 + x1^2 + x2^2 + x1*x2, 
#                   data=train,
#                   scale=FALSE,
#                   type='C-classification',
#                   shrinking=FALSE,
#                   kernel='linear', 
#                   cost=10)
# 
# grid_poly_predictions <- predict(svm_poly, newdata = test_grid)

```

```{r, echo=F}
# plot predictions for linear svm
test_grid_poly %>% 
    mutate(y_pred = grid_poly_predictions) %>% 
    ggplot() +
    geom_point(aes(x=x1, y=x2, color=y_pred), alpha=.3) +
    geom_point(data=train, aes(x=x1, y=x2, color=y, shape=y)) +
    theme(panel.background = element_blank(),
          legend.position="none") +
    ggtitle('SVM fit with degree 2 polynomial variable transformation')
```


Why stop at degree two polynomials -- why not add 3, 4, ... 412, etc? Two issues come up when we add more and more non-linear variables

- **overfitting**: if we add enough non-linear terms we might overfit the data
- **computational cost**: the more variables we add, the more computational costs we will incur (e.g. time + memory). If we add enough variables the computer may run out of memory and/or it might take until the end of the universe for the algorithm that fits SVM to stop.


We can use cross-validation to (attempt) to solve the problem of overfitting. There is not an immediately obvious way to fix the computational problem. Fortunately, there is a mathematical trick that can significantly reduce the computational cost.


## Kernel (implicit variable transformation)

See section 9.3.2 from [ISLR](http://www-bcf.usc.edu/~gareth/ISL/) for the details about Kernel methods.

[Kernel methods](https://en.wikipedia.org/wiki/Kernel_method) (also known as the *kernel trick*) is a trick that reformulates the mathematical problem of fitting a linear classifier to many non-linear variables into an easier to solve problem. This many not sound like much, but it makes more sophisticated algorithms usable in many settings where they would otherwise not be feasible. The key idea is that 

- many algorithms (such as SVM) rely only on the *distance* between *each pair of data points*.

This means that in order to fit SVM we only need to compute the distance between each pair of data points. There are two important consequences

1. If we can compute the distance between points cheaply then we can fit SVM more quickly.
2. If we have the ability to compute a "distance" between pairs of objects then we can use SVM.

The first point is the focus of this lecture: kernels = easier to compute non-linear version of SVM.

The second point means we can apply SVM to non-standard data such as: [stirngs](https://en.wikipedia.org/wiki/Text_mining), [graphs](https://en.wikipedia.org/wiki/Network_theory) and [images](https://en.wikipedia.org/wiki/Digital_image_processing. For exmaple, there are ways of measuring how similar two strings are (called [string kernels](https://en.wikipedia.org/wiki/String_kernel)). If we have a bunch of strings (e.g. Yelp restaurant reviews) and a bunch of labels (e.g. good/bad) we can use a SVM with a string kernel to build a string classifier. This idea can be applied in many settings such as [protien classification](http://summarization.com/~radev/767w10/papers/Week05/SimilarityKernels/leslie.pdf) (DNA can be viewed as a long string). Other kernel examples include [image kernels](http://setosa.io/ev/image-kernels/) and [graph kernels](https://en.wikipedia.org/wiki/Graph_kernel).

## Polynomial kernel

Suppose we have $n$ data points $x_1, \dots, x_n$ and $d$ variables (i.e. $x_i \in \mathbb{R}^d$). A kernel is a function that takes two data vectors and computes a measure of distance (or equivalently a measure of similarity) between the two points. For example, the degree $m$ polynomial kernel is defined as 

$$K(a, b) = (a^T b + 1)^m$$

where $a, b \in \mathbb{R}^d$ are two vectors and $a^Tb$ is the dot product between the two vectors. Sometimes the polynomial kernel is defined with another parameter $c \in \mathbb{R}$ that replaces the 1 i.e. $(a^T b + c)^m$. Or even a third parameter $\gamma \in \mathbb{R}$ that scales the dot product i.e. $(\gamma a^T b + c)^m$.

Using a degree $m$ polynomial kernel on the original data set is equivalent to adding all polynomial transformations up to degree $m$  of the original data. It's not obvious this is true, but the [wikipedia page on polynomial kernels](https://en.wikipedia.org/wiki/Polynomial_kernel) illustrates this fact with a degree two example.

## Computational complexity

This brief section requires some knowledge of [computational complexity](https://en.wikipedia.org/wiki/Analysis_of_algorithms) (e.g. bit O notation) to appreciate. You can skip this section if you want.

Why does this help? Suppose we have $d$ variables. Suppose we add a ton of transformed variables into the model and now have $D$ variables in the model where $d < D$. For example, if we add all quadratic transformations of our variables we will have $D = \frac{d(d+1)}{2}$ variables in the model. Computing the distance between two data points in the transformed space requires $O(D)=O(d^2)$ computations (e.g. a dot product in $\mathbb{R}^D$). However, computing the polynomial above requires $O(d)$ computations (e.g. a dot product in $\mathbb{R}^d$). 

For a quadratic kernel we go from $O(d^2)$ to $O(d)$ to compute distances between data points. For larger degree polynomials $D$ is an even worse function of $d$ (not hard to work out exactly what it is). However, computing the polynomial kernel is always an order $O(d)$ operations. 

If you want to learn more about how the SVM optimization problem is solved with Kernels read sections 8 and 9 from [Andrew Ng's notes](http://cs229.stanford.edu/notes/cs229-notes3.pdf) and the [Sequentioa Minimization Optimization](https://en.wikipedia.org/wiki/Sequential_minimal_optimization).


## Kernel SVM in R

Thanks to the `e1071` package it's easy to use Kernel SVMs in R. Below we fit SVM with a degree 2 polynomial kernel.

```{r}
# fit SVM
svm_kern2 <- svm(y ~ ., 
                  data=train,
                  cost=10,
                  kernel='polynomial', # use a polynomial kernel
                  degree = 2, # degree two polynomial
                  gamma=1, # other kernel parameters
                  coef0 =1, # other kernel parameters
                  scale=FALSE,
                  type='C-classification',
                  shrinking=FALSE)

kern2_predictions <- predict(svm_kern2, newdata = test_grid)
```


```{r, echo=F}
# plot predictions for kerenl svm
test_grid %>% 
    mutate(y_pred = kern2_predictions) %>% 
    ggplot() +
    geom_point(aes(x=x1, y=x2, color=y_pred), alpha=.3) +
    geom_point(data=train, aes(x=x1, y=x2, color=y, shape=y)) +
    theme(panel.background = element_blank(),
          legend.position="none") +
    ggtitle('SVM fit for degree 2 polynomial kernel')
```

The above plot should look the same as the previous plot when we explicitly added degree two variable transformations into the data. What happens when we try a degree 100 polynomial kernel?

```{r, echo=F}
# fit SVM
svm_kern <- svm(y ~ ., 
                  data=train,
                  cost=10,
                  kernel='polynomial', # use a polynomial kernel
                  degree = 100, # degree two polynomial
                  gamma=1, # other kernel parameters
                  coef0 =1, # other kernel parameters
                  scale=FALSE,
                  type='C-classification',
                  shrinking=FALSE)


test_grid <- expand.grid(x1 = seq(-10, 10, length = 100),
                         x2 = seq(-10, 10, length = 100)) %>% 
            as_tibble()


kern_predictions <- predict(svm_kern, newdata = test_grid)

# plot predictions for kerenl svm
test_grid %>% 
    mutate(y_pred = kern_predictions) %>% 
    ggplot() +
    geom_point(aes(x=x1, y=x2, color=y_pred), alpha=.3) +
    geom_point(data=train, aes(x=x1, y=x2, color=y, shape=y)) +
    theme(panel.background = element_blank(),
          legend.position="none") +
    ggtitle('SVM fit for degree 100 polynomial kernel')
```


There are many kernel functions you might use. Probably the most popular kernel is the [radial basis](https://en.wikipedia.org/wiki/Radial_basis_function_kernel) (aka Gaussian kernel) which you will try on the homework. Different kernels correspond to different explicit variable transformations.

So how do you select the parameters for the kernel? The basic polynomial kernel has one parameter (the degree). It could have up to three parameters (i.e. $\gamma$ and $c$). You still need to select the cost value parameters $C$. Typically, cross-validation (specifically grid-search cross-validation) is used to pick all parameters. In particular

1. Select a sequence of $C$ values (e.g. $C = 1e-5, 1e-4, \dots, 1e5$).
2. Select a sequence of degrees (e.g. $d = 1, 2, 5, 10, 20, 50, 100$).
3. For each pair of $C, d$ values (think a grid) use cross-validation to estimate the test error (originally cross validation had 2 for loops, now it has 3 for loops).
4. Select the pair $C, d$ values that give the best cross-validation error

This should feel some what clunky -- because it is. There are other fancier ways of selecting many tuning parameters (e.g. [Bayesian optimization](https://arxiv.org/pdf/1206.2944.pdf)), but cross-validation is used fairly often.

The code to do this will start to get a little uglier. Luckily someone made an R package that tunes many machine learning models.

## Tuning SVM with the caret package

The `caret` package is built to make fitting models much easier (caret stands for **C**lassification **A**nd **RE**gression **T**raining). The package even [comes with book on how to use it](http://topepo.github.io/caret/index.html).

[TODO] coming soon....

# **Classification metrics**
[TODO] coming soon
- misclassification error
- precision/recall
- F1 score
- ROC AUC

